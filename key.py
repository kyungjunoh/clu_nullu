import pickle
import numpy as np
import torch
from sklearn.cluster import KMeans
from numpy.linalg import norm
import os
import sys
from sklearn.preprocessing import normalize

# -------------------------------------------------------------------------
# 1. ë°ì´í„° ë¡œë“œ ë° ì°¨ì´ ë²¡í„° ê³„ì‚° í•¨ìˆ˜ (ë””ë²„ê¹… ê¸°ëŠ¥ ì¶”ê°€ë¨)
# -------------------------------------------------------------------------
def load_and_compute_diff(pkl_path, layer_to_visualize, feature_key='hidden_states', max_samples=2000):
    if not os.path.exists(pkl_path):
        print(f"ğŸ”´ [ì˜¤ë¥˜] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pkl_path}")
        return None

    print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘... {pkl_path}")
    with open(pkl_path, 'rb') as file:
        data = pickle.load(file)

    pos_features = []
    neg_features = []
    
    error_count = 0

    for i, entry in enumerate(data):
        try:
            feature_vector = entry[feature_key][layer_to_visualize].numpy()
            
            if entry['label'] == 0: # 0 = Hallucination
                pos_features.append(feature_vector)
            else: # 1 = Truthful
                neg_features.append(feature_vector)
        except Exception as e:
            if error_count < 5:
                print(f"âš ï¸ [ë°ì´í„° ì²˜ë¦¬ ì—ëŸ¬] ìƒ˜í”Œ {i}: {e}")
            error_count += 1
            continue

    # ì§ ë§ì¶”ê¸° ë° ìƒ˜í”Œ ìˆ˜ ì¡°ì ˆ
    min_len = min(len(pos_features), len(neg_features))
    if min_len == 0:
        print("ğŸ”´ [ì˜¤ë¥˜] ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return None
        
    if max_samples:
        min_len = min(min_len, max_samples)
    
    pos_features = pos_features[:min_len]
    neg_features = neg_features[:min_len]

    # â˜…â˜…â˜… [ìˆ˜ì •ë¨] dtype=np.float32 ì¶”ê°€ â˜…â˜…â˜…
    X_pos = np.array(pos_features, dtype=np.float32)
    X_neg = np.array(neg_features, dtype=np.float32)

    X_diff = X_pos - X_neg
    X_diff = normalize(X_diff, norm='l2')
    
    print(f"âœ… ì°¨ì´ ë²¡í„° ê³„ì‚° ì™„ë£Œ: {X_diff.shape} (Type: {X_diff.dtype})")
    return X_diff

# -------------------------------------------------------------------------
# 2. ë¶„ì„ í•¨ìˆ˜ë“¤
# -------------------------------------------------------------------------
def compute_cosine_similarity(v1, v2):
    return np.dot(v1, v2) / (norm(v1) * norm(v2))

def compute_projection_residual(vectors, subspace_vectors):
    # â˜…â˜…â˜… [ìˆ˜ì •ë¨] ì•ˆì „í•˜ê²Œ float32ë¡œ í•œë²ˆ ë” ë³€í™˜ â˜…â˜…â˜…
    vectors = vectors.astype(np.float32)
    subspace_vectors = subspace_vectors.astype(np.float32)

    # 1. ë¶€ë¶„ê³µê°„ì˜ ê¸°ì €(Basis) ì°¾ê¸° (SVD)
    # ì´ì œ float32ì´ë¯€ë¡œ ì—ëŸ¬ê°€ ë‚˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    U, S, Vt = np.linalg.svd(subspace_vectors.T, full_matrices=False)
    
    k = 4  # ìƒìœ„ kê°œ ë°©í–¥
    Vk = U[:, :k] 
    
    # 2. íˆ¬ì˜ í–‰ë ¬ P = V V^T
    P = Vk @ Vk.T
    
    # 3. ì˜ê³µê°„ íˆ¬ì˜ (I - P)
    I = np.eye(P.shape[0], dtype=np.float32)
    Null_P = I - P
    
    # 4. íˆ¬ì˜ í›„ í¬ê¸° ë¹„ìœ¨ ê³„ì‚°
    original_norms = np.linalg.norm(vectors, axis=1).mean()
    projected_vectors = vectors @ Null_P
    projected_norms = np.linalg.norm(projected_vectors, axis=1).mean()
    
    ratio = projected_norms / original_norms
    return ratio

# -------------------------------------------------------------------------
# 3. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# -------------------------------------------------------------------------
def main():
    # íŒŒì¼ ê²½ë¡œ í™•ì¸ í•„ìˆ˜!
    PKL_FILE_PATH = "/data/Nullu/output/LLaVA-7B/lure_train_0_activations.pkl"
    
    # â˜… ì¤‘ìš”: t-SNEì—ì„œ ë¶„ë¦¬ê°€ ì˜ ë˜ì—ˆë˜ 'hidden_states'(ë§ˆì§€ë§‰ í† í°)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.
    # ë…¼ë¬¸ì²˜ëŸ¼ 'mean'ì„ ì“°ë ¤ë©´ 'hidden_states_mean'ìœ¼ë¡œ í•˜ì‹œë©´ ë©ë‹ˆë‹¤.
    KEY = 'hidden_states_mean' 
    LAYER = 31
    
    print("=== ë¶„ì„ ì‹œì‘ ===")
    X_diff = load_and_compute_diff(PKL_FILE_PATH, LAYER, KEY, max_samples=5000)
    
    if X_diff is None:
        print("âŒ ë¶„ì„ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    # K-Meansë¡œ ê·¸ë£¹ ë¶„ë¦¬ (k=2)
    print(f"ğŸŒ€ K-Means í´ëŸ¬ìŠ¤í„°ë§ (k=2) ìˆ˜í–‰ ì¤‘...")
    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10).fit(X_diff)
    labels = kmeans.labels_
    
    group_A = X_diff[labels == 0]
    group_B = X_diff[labels == 1]
    
    print(f"   - Group A ìƒ˜í”Œ ìˆ˜: {len(group_A)}")
    print(f"   - Group B ìƒ˜í”Œ ìˆ˜: {len(group_B)}")
    
    # --- ì‹¤í—˜ 1: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ---
    mean_A = np.mean(group_A, axis=0)
    mean_B = np.mean(group_B, axis=0)
    
    similarity = compute_cosine_similarity(mean_A, mean_B)
    print("\n" + "=" * 40)
    print(f"ğŸ§ª [ì‹¤í—˜ 1] ë‘ ê·¸ë£¹ í‰ê·  ë²¡í„°ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„")
    print("=" * 40)
    print(f"â–¶ ê²°ê³¼: {similarity:.4f}")
    print(f"   (í•´ì„: 0ì— ê°€ê¹Œìš°ë©´ 'ì§êµ(ë‹¤ë¥¸ ë°©í–¥)', 1ì— ê°€ê¹Œìš°ë©´ 'ê°™ì€ ë°©í–¥')")
    
    # --- ì‹¤í—˜ 2: êµì°¨ íˆ¬ì˜ (Cross-Projection) ---
    remaining_ratio_B_by_A = compute_projection_residual(group_B, group_A)
    remaining_ratio_A_by_B = compute_projection_residual(group_A, group_B)
    
    print("\n" + "=" * 40)
    print("ğŸ§ª [ì‹¤í—˜ 2] êµì°¨ íˆ¬ì˜ í…ŒìŠ¤íŠ¸ (Cross-Projection)")
    print("=" * 40)
    print(f"â–¶ Aì˜ ë°©íŒ¨ë¡œ Bë¥¼ ë§‰ì•˜ì„ ë•Œ, Bê°€ ì‚´ì•„ë‚¨ì€ ë¹„ìœ¨: {remaining_ratio_B_by_A * 100:.2f}%")
    print(f"â–¶ Bì˜ ë°©íŒ¨ë¡œ Aë¥¼ ë§‰ì•˜ì„ ë•Œ, Aê°€ ì‚´ì•„ë‚¨ì€ ë¹„ìœ¨: {remaining_ratio_A_by_B * 100:.2f}%")
    print("   (í•´ì„: 100%ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì„œë¡œ ì „í˜€ ë§‰ì§€ ëª»í•˜ëŠ” 'ë…ë¦½ì ì¸ ë°©í–¥'ì„)")

# â˜…â˜…â˜… ì´ ë¶€ë¶„ì´ ë¹ ì ¸ ìˆì–´ì„œ ì‹¤í–‰ì´ ì•ˆ ë˜ì—ˆë˜ ê²ƒì…ë‹ˆë‹¤! â˜…â˜…â˜…
if __name__ == "__main__":
    main()